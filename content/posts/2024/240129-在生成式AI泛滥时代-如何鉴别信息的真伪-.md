---
title: 在生成式AI泛滥时代，如何鉴别信息的真伪？
date: 2024-01-29 09:44:00+08:00
categories: [公众号]
tags: [生成式AI, 信息鉴别, 科学伦理, 媒体素养, 信息素养]
slug: ai-era-information-verification
---

<div class="p-3 text-center">
  <img class="img-fluid" src="/images/2024/0129/01.png" alt="题图" style="max-width:640px">
  <div><small>（题图由AI生成）</small></div>
</div>

在生成式AI泛滥的时代，鉴别信息的真伪变得日益重要。受过系统性科研训练的人，通常采用一种有效的方法：追溯信息的原始来源及支持证据。即便AI技术正在持续大幅降低信息伪造的成本，面临如此挑战，这种方法也仍然是有效的。

在科研写作中，陈述任何事实时都必须提供信息来源，并且通常都要求引用第一手资料，即得出该事实结论的原始研究工作。这一规则确保了读者能够根据引用关系，追踪到原始文献，从而理解支持某一事实的证据有多充分。这种追本溯源的能力，不仅是科研人员的基本功，也是硕博生培养的重点之一。

相比之下，一些不负责任的媒体在报道时往往隐藏信息来源，采取各种手法进行裁剪，其内容就显得很不可信了。但大多数观众和读者并未养成追溯信息来源的习惯，因而容易被“劲爆”的内容吸引，助长了虚假信息的传播。这种情况在生成式AI泛滥的背景下变得更加普遍。

回顾十几年前，我曾参加考研英语冲刺班。当时我的英文水平很烂，写作文时经常陷入“便秘”，其他同学应该也类似。培训班教我们使用各种废话句式和虚构引用，例如加上“As all around the world known”（众所周知）或“Nothing could be further from the truth”（真得不能再真了）等词句来凑足字数，或是加上“As Dr. Wang said,”（如王博士所说）或“As Professor Zhang said,”（如张教授所说）来“增强可信度”，按培训班老师当年的说法，“王博士是谁，张教授是谁，没人知道，也不重要”。为了拥有学习如何成为“诚信”科研人员的资格，却需要通过这种明显不诚信的方法来通过考试，想想也挺讽刺的。

幸好，后来的科研训练纠正了我的这一行为，避免我在歧途上越走越远。但我们都知道，考研的录取率其实是非常低的，大量参与过该写作学习的人，最终都未必有机会被纠正行为，这种连锁危害，也必然已经波及到更深远的范围。在如今生成式AI技术被各行各业追捧得火热之际，缺乏道德底线的约束，只会让问题变得更加棘手。比如，生成式AI可以轻松地生成看似专业的支持证据。除非我们去专业的文献数据库中查询并逐一确认信息来源，否则几乎无法区分真伪。这不仅加重了读者的负担，而且作为科研工作者，我们也不可能对每件事都深入本源进行验证。我们都精力有限、时间有限、眼界有限，必须依赖前人的工作，才能勉强推进一点点科研的边界，如今却要消耗更多，只为确认自己正在构建的大厦，并非构筑于沙地。这也是很多人极力反对在科研发表过程中过度使用AI的重要原因。

事实上，这种追本溯源的过程，正是一种知识推理，也恰恰是大型语言模型所擅长的。如果AI不产生幻觉（Hallucinations，即胡编乱造），我们最终还是会依赖它来判断信息的真伪。追溯并确认证据链条上每个环节，是确保信息真实可信的基本原理。尽管如此，完全编造整个证据链条并非不可能，只不过需要消耗更大的资源（如算力）。真实和虚伪之间的博弈，最终也会演变成算力等资源的角逐，这听起来挺魔幻的。
这指向了我们也许需要另一种东西：可信的、不可随意篡改的信息长期保存机制。这在现实中类似于科研论文发表体系（虽然这个体系其实也受到了恶意侵染；许多人正在致力于解决这个问题），但我们需要将其扩展到更广泛的领域，拥有更完善的检索功能和实时更新能力。这听起来可能像是区块链或Web3.0的概念，但它们该以怎样的方式得以正确应用，最终效果到底如何，仍需时间来检验。

<div class="p-5 text-center">--- END ---</div>

<i><b>注：</b>本文首发表于[“不靠谱颜论”公众号](https://mp.weixin.qq.com/s/pRO_9NxqsmyiOGGWZYrR_A)，并同步至本站。</i>
